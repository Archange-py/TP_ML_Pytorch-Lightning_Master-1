{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXfaiY3oQxlA"
   },
   "source": [
    "# Unsupervised Learning and AutoEncoders.\n",
    "The goal of this notebook is to understand and handle AutoEncoders using Pytorch.\n",
    "\n",
    "It is recommended to run the notebook on Colab over a GPU device ([how to use a GPU on colab ](https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/colab.html)).\n",
    "\n",
    "Inspired by : https://deeplearning.neuromatch.io/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial1.html\n",
    "\n",
    "## Guidelines :\n",
    "Carefuly read the given code then\n",
    "*   complete cells marked with a TODO.\n",
    "*   answer questions marked with a QUESTION .\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "---\n",
    "### 1) Initialisation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3erI39SzQqDd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCJ_pBNsRDEH",
    "outputId": "02053064-a723-4d24-b3ea-125809de5fc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "# Set numpy seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set torch seeds\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set device:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRdN_G6bRGr3"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "### 1.1) The dataset\n",
    "---\n",
    "\n",
    "We are going to apply our AutoEncoder to the MNIST dataset, we can start by downloading it from Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzg1ih-kRFQw",
    "outputId": "44883b11-e61d-40b1-e708-e775772852f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:05<00:00, 1762136.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 733875.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 3419092.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 842831.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Downloading dataset and creating Torch Dataloaders:\n",
    "\n",
    "# Download the MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root = \"./data\",\n",
    "                         train = True,\n",
    "                         download = True,\n",
    "                         transform = transforms.ToTensor())\n",
    "val_dataset = datasets.MNIST(root = \"./data\",\n",
    "                         train = False,\n",
    "                         download = True,\n",
    "                         transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tewfuAjLRUDC"
   },
   "source": [
    "The following list will be useful for plotting specific digits (see 3) Generative Models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "W2wArv2ZRXS4",
    "outputId": "4209d3d7-7c4d-4eef-9013-848dec0a7165"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_digits_idx = []\n",
    "for digit in range(10):\n",
    "  idx_image = 0\n",
    "  while val_dataset[idx_image][1] != digit :\n",
    "    idx_image+=1\n",
    "  all_digits_idx.append(idx_image)\n",
    "\n",
    "fig,axs = plt.subplots(1,10, sharey=True, figsize=(16,8))\n",
    "for i,idx in enumerate(all_digits_idx):\n",
    "  axs[i].imshow(val_dataset[idx][0].view(28,28))\n",
    "  axs[i].set_title(f\"Label: {val_dataset[idx][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7Mlg__QRlnt"
   },
   "source": [
    "Note that for the rest of this notebook, we will be exploring the realm of Unsupervized Learning approaches. Althugh MNIST is a dataset comrpised of images as well as lassociated labels, we will intentionally disregard the labels, and focus solely on the pixel information. \n",
    "\n",
    "With this in mind, the goal of the notebook is to explore how we can work with only pixel values. Several possibilities arise, we focus on AutoEncoders and the concept of *Latent Spaces*. \n",
    "\n",
    "Without loss of generality, an AutoEncoder is comprised of **encoder**, a **bottleneck** and a **decoder** :\n",
    "*  The encoder maps data from a high-dimensional input to the bottleneck where the sub-space is the smallest.\n",
    "* The decoder converst the encoded input back to the original input space.\n",
    "* The space of the bottleneck is what we commonly refer to as the **latent space**. It is usually much smaller than the input space, but more informative. It can be seen as a compressed version of the image space in our case. Usually AutoEncoders look something like this:\n",
    "\n",
    "![AE.webp](data:image/webp;base64,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)\n",
    "\n",
    "\n",
    "(Image from [this article](https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df))\n",
    "\n",
    "Thoughouth the notebook, ty to have a look at this figure and identify the different components of our networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3Y7B0iuRtI7"
   },
   "source": [
    "---\n",
    "### 1.2) Helper Functions\n",
    "---\n",
    "\n",
    "Take time to understand and complete the following helper functions. What do they do?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I3x5xswVRe3e"
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(autoencoder, train_dataset, val_dataset, batch_size, device, epochs=20, lr=1e-3, weight_decay=1e-5):\n",
    "  \"\"\"\n",
    "  Function to train a given autoencoder\n",
    "\n",
    "  Input:\n",
    "    autoencoder: a torch Autoencoder \n",
    "    train_dataset: Torch dataset containing training samples.\n",
    "    val_dataset: Torch dataset containing validation samples.\n",
    "    batch_size: Batch size to construct dataloaders.\n",
    "    device: Device to run model on.\n",
    "    epochs: Number of epochs [default: 20]\n",
    "    lr: Learning rate for ADAM optimizer [default: 1e-3]\n",
    "    weight_decay: Weight decay factor for ADAM optimizer [default: 1e-5]\n",
    "\n",
    "  Returns:\n",
    "    loss_train: List of losses over each epoch during training phase.\n",
    "    loss_val: List of losses over each epoch during validation phase.\n",
    "  \"\"\"\n",
    "  # Set model to device, construct dataloaders and optimizer\n",
    "  autoencoder.to(device)\n",
    "  optimizer = torch.optim.Adam(autoencoder.parameters(),\n",
    "                           lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "  criterion = nn.MSELoss()\n",
    "  train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                     batch_size = batch_size,\n",
    "                                     shuffle = True)\n",
    "  val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                     batch_size = batch_size,\n",
    "                                     shuffle = False)\n",
    "  # Initialize empty loss lists\n",
    "  loss_train, loss_val = [],[]\n",
    "\n",
    "  # Run training loop\n",
    "  for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "    ####### Train loop #######\n",
    "    autoencoder.train()\n",
    "    running_loss = 0.0\n",
    "    for image, _ in train_loader:\n",
    "      # ===================forward=====================\n",
    "      image = image.to(device)\n",
    "      reconstructed_image = autoencoder(image)\n",
    "      loss = criterion(reconstructed_image.view(batch_size, -1), image.view(batch_size, -1))\n",
    "      # ===================backward====================\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Log the batch loss\n",
    "      running_loss += loss.detach() * image.size(0)\n",
    "    # Append the epoch's loss:\n",
    "    loss_train.append(running_loss.cpu()/len(train_dataset))\n",
    "\n",
    "    ####### Val loop #######\n",
    "    autoencoder.eval()\n",
    "    running_loss = 0.0\n",
    "    for image, _ in val_loader:\n",
    "      ### TODO: Complete the validation loop of the function ####\n",
    "      # ===================forward=====================\n",
    "      image = image.to(device)\n",
    "      reconstructed_image = autoencoder(image)\n",
    "      loss = criterion(reconstructed_image.view(batch_size, -1), image.view(batch_size, -1))\n",
    "\n",
    "      # Log the batch loss\n",
    "      running_loss += loss.detach() * image.size(0)\n",
    "    # Append the epoch's loss:\n",
    "    loss_val.append(running_loss.cpu()/len(val_dataset))\n",
    "\n",
    "  autoencoder.to('cpu')\n",
    "  return loss_train, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cEh-0zDhR1vc"
   },
   "outputs": [],
   "source": [
    "def plot_samples(autoencoder, dataset, num_to_plot=10):\n",
    "  fig, axs = plt.subplots(2,10, figsize=(80,10))\n",
    "  for i in range(num_to_plot):\n",
    "    sample = dataset[np.random.randint(len(dataset))][0].view(-1,1,28,28)\n",
    "    reconstruction = autoencoder(sample)\n",
    "\n",
    "    axs[0][i].imshow(sample.view(28, 28))\n",
    "    axs[1][i].imshow(reconstruction.detach().view(28, 28))\n",
    "  plt.subplots_adjust(wspace=0, hspace=0)\n",
    "  plt.show()\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce3KwOpyVGhX"
   },
   "source": [
    "---\n",
    "### 1.3 Hyperparameters tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZHQJIoWpR71V"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters for all of our models. You may change this at will after running the notebook\n",
    "\n",
    "latent_dim = 20 # Size of our latent dimension\n",
    "batch_size = 64 \n",
    "num_epochs = 15\n",
    "learning_rate = 1e-3\n",
    "weight_decay_value = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQi7xqEUR9bv"
   },
   "source": [
    "---\n",
    "## 2) The Models\n",
    "---\n",
    "\n",
    "We can begin by simply implementing a Linear AutoEncoder. It is composed of two fully connected layers, one for encoding our input and one for decoding our latent space.\n",
    "\n",
    " In order to ensure strict linearity for now, no activation functions are used.\n",
    "\n",
    "---\n",
    " ### 2.1) Linear AutoEncoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oqQOZv-ISKGK"
   },
   "outputs": [],
   "source": [
    "class LinearAutoEncoder(nn.Module):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(LinearAutoEncoder, self).__init__()\n",
    "\n",
    "    # Encoder layer (a linear mapping from input size to hidden_dimension)\n",
    "    self.linear_encoder = nn.Linear(28 * 28, latent_dim)\n",
    "\n",
    "    # Decoder layer (a linear mapping from hidden_dimension to input size)\n",
    "    self.linear_decoder = nn.Linear(latent_dim, 28 * 28)\n",
    "\n",
    "  def encode(self, x):\n",
    "    # Encode the input sample\n",
    "    latent = self.linear_encoder(x.view(x.size(0), -1))\n",
    "    return latent\n",
    "\n",
    "  def decode(self, latent):\n",
    "    # Decode the latent vector\n",
    "    x_prime = self.linear_decoder(latent).view(-1,1,28,28)\n",
    "    return x_prime\n",
    "\n",
    "  def forward(self, x):\n",
    "    latent = self.encode(x)\n",
    "    return self.decode(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0w7EgVxSOle",
    "outputId": "1dd5f62d-7482-4dc9-9a44-b058529ced68"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Construct AE\u001b[39;00m\n\u001b[0;32m      2\u001b[0m linear_ae \u001b[38;5;241m=\u001b[39m LinearAutoEncoder(latent_dim)\n\u001b[1;32m----> 4\u001b[0m summary(\u001b[43mlinear_ae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Blondel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Blondel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Blondel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Blondel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Blondel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 293\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Construct AE\n",
    "linear_ae = LinearAutoEncoder(latent_dim)\n",
    "\n",
    "summary(linear_ae.to('cuda'), (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "0kSRfoa0SjG2",
    "outputId": "5e8f47e1-1b51-459c-839c-46749de5440d"
   },
   "outputs": [],
   "source": [
    "##### Train model:\n",
    "\n",
    "### TODO:  Train the AE using the previously defined hyper-parameters ###\n",
    "\n",
    "#???\n",
    "\n",
    "### TODO: Plot the Reconstruction train and validation losses of the Linear AE ###\n",
    "\n",
    "#???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "Bhwqrh92SsMA",
    "outputId": "859fb3c5-0e4b-4bd9-91a0-c80074f6b439"
   },
   "outputs": [],
   "source": [
    "# Plot some reconstructions:\n",
    "plot_samples(linear_ae, val_dataset, num_to_plot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A9KG_5YSvSv"
   },
   "source": [
    "#### QUESTION\n",
    " What do you think of these sample reconstructions? What easy steps could you implement in order to obtain more accurate reconstructions?\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjfV6kj4S2Jw"
   },
   "source": [
    "TODO: They are not visually pleasing or accurate, but are still impressive for such a basic model. The model can clearly differentiate between digits, but inter-digit variation seems to be blurry. We could add more layers to the encoder and decoder , but activation functions would then be necessary and thus the model would not be linear strictly speaking. Furthermore, an output activation could help force the network to at least get the correct intensity range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5-fSQnMS5Y9"
   },
   "source": [
    "---\n",
    "### 2.2) Convolutional AutoEncoder (Non Linear)\n",
    "---\n",
    "In order to implement a Convolutional AutoEncoder, we will be using Transpose Convolutions in order to upsample our latent space.\n",
    "\n",
    "#### QUESTION\n",
    "\n",
    " Using [Pytorch's documentation](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) and [this explanation](https://d2l.ai/chapter_computer-vision/transposed-conv.html), explain in your own words how a Transpose convolution works. Does it correspond to a \"deconvolution\" in the sens of the inverse of the convolution operator?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3o2BN9JTHeZ"
   },
   "source": [
    "TODO: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRXjCy3ZTL_i"
   },
   "source": [
    "The following is an example implementation of a Convolutional AE using pytorch. It takes an MNIST image (1,28,28) image, convolves it to a (64,1,1) feature map\n",
    "\n",
    "*   It takes an MNIST image (1,28,28) image,\n",
    "*   Convolves it to a (64,1,1) feature map,\n",
    "*   Flattens the feature map, into a vector of shape 64,\n",
    "*   Reduces this to the given latent_dimension size,\n",
    "*   Then decodes the latent vector back to an image.\n",
    "\n",
    "Complete the class implementation in order to have an AE which functions as aforementioned. (You may want to use [Pytroch's documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) for a refresher on convolutional arithmetic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9Il7mRJSuDR"
   },
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self,latent_dim):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        # Input (batch_size, 1, 28, 28)\n",
    "        self.encoder = nn.Sequential(\n",
    "            ### TODO: Complete the missing values ###\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), # (batch_size, 16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # (batch_size, 32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7), # (batch_size, 64, 1, 1)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "            )\n",
    "        \n",
    "        ### TODO: Complete the missing values ###\n",
    "        self.fc_encoder = #...\n",
    "        self.fc_decoder = #...\n",
    "        \n",
    "        \n",
    "        #64, 1, 1\n",
    "        self.decoder = nn.Sequential(\n",
    "            ### TODO: Complete the missing values ###\n",
    "            nn.Unflatten(dim=-1, unflattened_size=(64,1,1)),\n",
    "            nn.ConvTranspose2d(64, 32, 7), # (batch_size, 32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # (batch_size, 16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), # (batch_size,1, 28, 28)\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        latent = self.fc_encoder(x)\n",
    "        return latent\n",
    "\n",
    "    def decode(self, latent):\n",
    "        s = F.relu(self.fc_decoder(latent))\n",
    "        x_prime = self.decoder(s)\n",
    "        return x_prime\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encode(x)\n",
    "        x_prime = self.decode(latent)\n",
    "        return x_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtrg6zCiTdF9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M40r63-fTdTD"
   },
   "source": [
    "#### QUESTION\n",
    " Why is the final activation of our network a Sigmoid? What other activation functions could we have used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXg8v3NaTkNs"
   },
   "source": [
    "TODO : ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zNE-72HdSyj5",
    "outputId": "e8373257-737f-4f11-c722-26a4c471e313"
   },
   "outputs": [],
   "source": [
    "# Construct AE\n",
    "conv_ae = ConvAutoEncoder(latent_dim)\n",
    "\n",
    "summary(conv_ae.to('cuda'), (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "ieBuwvTJTthf",
    "outputId": "56ca8b41-1c4c-485e-a500-ce98f96d1202"
   },
   "outputs": [],
   "source": [
    "##### Train model:\n",
    "\n",
    "### TODO:  Train the AE using the previously defined hyper-parameters ###\n",
    "conv_ae_loss_train, conv_ae_loss_val = train_autoencoder( #...\n",
    "    \n",
    "plt.plot(conv_ae_loss_train, label=\"Train loss\")\n",
    "plt.plot(conv_ae_loss_val, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(conv_ae_loss_train, label=\"Conv AE\")\n",
    "plt.plot(linear_ae_loss_train, label=\"Linear AE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hd5bNMFTzag"
   },
   "source": [
    "#### QUESTION\n",
    " Which of both models performs better? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3awnAgLMT2xN"
   },
   "source": [
    "TODO : ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "ikpBQx-4T6in",
    "outputId": "8049c07f-13d8-49b5-feaf-07f9c81ef107"
   },
   "outputs": [],
   "source": [
    "# Plot some reconstructions:\n",
    "plot_samples(conv_ae, val_dataset, num_to_plot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-k6zoo3T92k"
   },
   "source": [
    "\n",
    "---\n",
    "### 2.3) Variational AutoEncoder (VAEs)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUqwtRnAUAhp"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Input: (batch_size, 1, 28, 28)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), #(batch_size, 16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), #(batch_size, 32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7), #(batch_size, 64, 1, 1)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "        \n",
    "        self.fc_decoder = nn.Linear(latent_dim, 64)\n",
    "        \n",
    "        \n",
    "        # Input: (batch_size, 64, 1, 1)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(dim=-1, unflattened_size=(64,1,1)),\n",
    "            nn.ConvTranspose2d(64, 32, 7), #(batch_size, 32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), #(batch_size, 16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), #(batch_size, 1, 28, 28)\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + (std * epsilon)\n",
    "        return z\n",
    "\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Get mu and logvar or reparam trick\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "\n",
    "        latent = self.reparameterize(mu, logvar)\n",
    "        return latent, mu, logvar\n",
    "\n",
    "    def decode(self, latent):\n",
    "        s = F.relu(self.fc_decoder(latent))\n",
    "        x_prime = self.decoder(s)\n",
    "        return x_prime\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent, mu, logvar = self.encode(x)\n",
    "       \n",
    "        x_prime = self.decode(latent)\n",
    "        return x_prime, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmnsNUKMUFaf"
   },
   "source": [
    "The following is a re-implementation of the train function for the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPDR9t5oUIOR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def VAE_loss_fn(mse_loss, mu, logvar, beta=1):\n",
    "    KL_Div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return mse_loss + beta*KL_Div\n",
    "\n",
    "\n",
    "\n",
    "def train_VAE(vae, train_dataset, val_dataset, batch_size, device, epochs=20, lr=1e-3, weight_decay=1e-5, beta=1):\n",
    "  \"\"\"\n",
    "    Function to train a given autoencoder\n",
    "\n",
    "    Input:\n",
    "      vae: a VAE model \n",
    "      train_dataset: Torch dataset containing training samples.\n",
    "      val_dataset: Torch dataset containing validation samples.\n",
    "      batch_size: Batch size to construct dataloaders.\n",
    "      device: Device to run model on.\n",
    "      epochs: Number of epochs [default: 20]\n",
    "      lr: Learning rate for ADAM optimizer [default: 1e-3]\n",
    "      weight_decay: Weight decay factor for ADAM optimizer [default: 1e-5]\n",
    "      beta: weight of Kullback-Leibler divergence in VAE loss function\n",
    "\n",
    "    Returns:\n",
    "      loss_train: List of losses over each epoch during training phase.\n",
    "      loss_val: List of losses over each epoch during validation phase.\n",
    "      mse_loss_train\n",
    "      mse_loss_val\n",
    "  \"\"\"\n",
    "  # Set model to device, construct dataloaders and optimizer\n",
    "  vae.to(device)\n",
    "  optimizer = torch.optim.Adam(vae.parameters(),\n",
    "                           lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "  MSE_criterion = nn.MSELoss(reduction='sum')\n",
    "  train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                     batch_size = batch_size,\n",
    "                                     shuffle = True)\n",
    "  val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                     batch_size = batch_size,\n",
    "                                     shuffle = False)\n",
    "  # Initialize empty loss lists\n",
    "  loss_train, loss_val = [],[]\n",
    "  mse_loss_train, mse_loss_val = [],[]\n",
    "  # Run training loop\n",
    "  for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "    ####### Train loop #######\n",
    "    vae.train()\n",
    "    running_loss, running_loss_mse = 0.0, 0.0\n",
    "    for image, _ in train_loader:\n",
    "        # ===================forward=====================\n",
    "      image = image.to(device)\n",
    "      reconstruction, mu, logvar  = vae(image)\n",
    "      # Compute loss elements :\n",
    "      mse_loss = MSE_criterion(reconstruction.view(batch_size, -1), image.view(batch_size, -1))\n",
    "      loss = VAE_loss_fn(mse_loss, mu, logvar, beta=beta)\n",
    "      # ===================backward====================\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Log the batch loss\n",
    "      running_loss += loss.detach() * image.size(0)\n",
    "      running_loss_mse += mse_loss.detach() * image.size(0)\n",
    "    # Append the epoch's loss:\n",
    "    loss_train.append(running_loss.cpu()/len(train_dataset))\n",
    "    mse_loss_train.append(running_loss_mse.cpu()/len(train_dataset))\n",
    "\n",
    "    ####### Val loop #######\n",
    "    vae.eval()\n",
    "    running_loss, running_loss_mse = 0.0, 0.0\n",
    "    for image, _ in val_loader:\n",
    "      # ===================forward=====================\n",
    "      image = image.to(device)\n",
    "      reconstruction, mu, logvar  = vae(image)\n",
    "      # Compute loss elements :\n",
    "      mse_loss = MSE_criterion(reconstruction.view(batch_size, -1), image.view(batch_size, -1))\n",
    "      loss = VAE_loss_fn(mse_loss, mu, logvar, beta=beta)\n",
    "\n",
    "      # Log the batch loss\n",
    "      running_loss += loss.detach() * image.size(0)\n",
    "      running_loss_mse += mse_loss.detach() * image.size(0)\n",
    "    # Append the epoch's loss:\n",
    "    loss_val.append(running_loss.cpu()/len(val_dataset))\n",
    "    mse_loss_val.append(running_loss_mse.cpu()/len(val_dataset))\n",
    "\n",
    "  vae.to('cpu')\n",
    "  return loss_train, loss_val, mse_loss_train, mse_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgTTlQBHUO_g",
    "outputId": "f0cd2a48-cafe-416d-8d50-f2fca3a08273"
   },
   "outputs": [],
   "source": [
    "# Construct AE\n",
    "vae_model = VAE(latent_dim)\n",
    "\n",
    "summary(vae_model.to('cuda'), (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "u91Z4l5vUcM6",
    "outputId": "76c5161a-489c-4ec6-a955-f120fc7de62f"
   },
   "outputs": [],
   "source": [
    "#### Train VAE\n",
    "\n",
    "### TODO:  Train the VAE using the previously defined hyper-parameters ###\n",
    "vae_loss_train, vae_loss_val, vae_loss_train_mse, vae_loss_val_mse = train_VAE(#...\n",
    "    \n",
    "plt.plot(vae_loss_train, label=\"Train loss\")\n",
    "plt.plot(vae_loss_val, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(vae_loss_train, label=\"VAE\")\n",
    "plt.plot(conv_ae_loss_train, label=\"Conv AE\")\n",
    "plt.plot(linear_ae_loss_train, label=\"Linear AE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R831DUjOUf4b"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### QUESTION\n",
    " Why does the plotted Loss seem to indicate that the VAE has a terrible reconstruction compared to the previous models? Is it the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpjRdJUwUjGx"
   },
   "source": [
    "TODO: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "wS8DlCgQUnhS",
    "outputId": "cf60f7e6-3a5a-45ec-96a8-4a299db8c0f1"
   },
   "outputs": [],
   "source": [
    "#Plot the VAE's training MSE and KLD loss seperately. Recall that VAE_loss = mse_loss + KL_Div (if you kept beta=1) ###\n",
    "plt.plot(vae_loss_train_mse, label=\"mse\")\n",
    "plt.plot([vae_loss_train[i] - vae_loss_train_mse[i] for i in range(len(vae_loss_train_mse)) ], label=\"KLD\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "EmraEMejUq0a",
    "outputId": "ddcd83f2-50ed-4bed-b622-51ba15a42a2a"
   },
   "outputs": [],
   "source": [
    "#Read the plot_samples helper function again, and complete the script in order to compare sample reconstructions from all three trained models  ###\n",
    "\n",
    "fig, axs = plt.subplots(4,10, figsize=(60,10))\n",
    "criterion = nn.MSELoss()\n",
    "for i in range(10):\n",
    "  # Get a random sample from the MNIST validation set.\n",
    "  sample = val_dataset[np.random.randint(len(val_dataset))][0].view(-1,1,28,28)\n",
    "\n",
    "  # Forward through trained models to get reconstructions.\n",
    "  reconstruction_linear_ae = linear_ae(sample) # TODO\n",
    "  reconstruction_conv_ae = conv_ae(sample) # TODO\n",
    "  reconstruction_vae, _, _ = vae_model(sample) # TODO\n",
    "\n",
    "  # Evaluate the reconstruction error.\n",
    "  lin_ae_mse = criterion(reconstruction_linear_ae,sample).item() # TODO\n",
    "  conv_ae_mse = criterion(reconstruction_conv_ae,sample).item() # TODO\n",
    "  vae_mse = criterion(reconstruction_vae,sample).item() # TODO\n",
    "\n",
    "  # Display everything\n",
    "  axs[0][i].imshow(sample.view(28, 28))\n",
    "  axs[1][i].imshow(reconstruction_linear_ae.detach().view(28, 28))\n",
    "  axs[1][i].set_title(str(lin_ae_mse))\n",
    "  axs[2][i].imshow(reconstruction_conv_ae.detach().view(28, 28))\n",
    "  axs[2][i].set_title(str(conv_ae_mse))\n",
    "  axs[3][i].imshow(reconstruction_vae.detach().view(28, 28))\n",
    "  axs[3][i].set_title(str(vae_mse))\n",
    "#plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVJ6B9lyWBvR"
   },
   "source": [
    "---\n",
    "## 3) Autoencoders as generative models\n",
    "---\n",
    "\n",
    "AutoEncoders are not only useful for dimensionality-reduction. They also encapsulate generating capabilities, and sometimes display interesting properties in terms of their Latent Space.\n",
    "\n",
    "#### QUESTION\n",
    " What does the following function do?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvvhQLlFWOml"
   },
   "source": [
    "TODO : ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "5cinKrBUWIk7",
    "outputId": "c57cf4e2-b0d0-499c-f7d6-3b2d205b8940"
   },
   "outputs": [],
   "source": [
    "def interpolate(ae, x_1, x_2, n=12):\n",
    "    if not isinstance(ae,VAE):\n",
    "      z_1 = ae.encode(x_1.view(-1,1,28,28))\n",
    "      z_2 = ae.encode(x_2.view(-1,1,28,28))\n",
    "    else:\n",
    "      z_1, mu_1, logvar_1 = ae.encode(x_1.view(-1,1,28,28))\n",
    "      z_2, mu_2, logvar_2 = ae.encode(x_2.view(-1,1,28,28))\n",
    "    image = np.zeros((28,28*n))\n",
    "    for i, t in enumerate(np.linspace(0, 1, n)):\n",
    "      if not isinstance(ae,VAE):\n",
    "        recons = ae.decode(z_1 + (z_2 - z_1)*t).view(28,28).detach().numpy()\n",
    "      else:\n",
    "        mu = mu_1 + (mu_2 - mu_1)*t\n",
    "        logvar = logvar_1 + (logvar_2 - logvar_1)*t\n",
    "        latent = ae.reparameterize(mu, logvar)\n",
    "        recons = ae.decode(latent).detach().numpy()\n",
    "      image[:, 28*i:28*(i+1)] = recons\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "idx_1, idx_2 = all_digits_idx[4], all_digits_idx[6]\n",
    "img_1, img_2 = val_dataset[idx_1][0], val_dataset[idx_2][0]\n",
    "\n",
    "interpolate(linear_ae, img_1, img_2)\n",
    "interpolate(conv_ae, img_1, img_2)\n",
    "interpolate(vae_model, img_1, img_2)\n",
    "\n",
    "# interpolate(model,image[0,:,:,:],image[3,:,:,:])\n",
    "# interpolate(model,image[3,:,:,:],image[2,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PXP3lF4WU8l"
   },
   "source": [
    "#### QUESTION\n",
    " What can you say abouth the latent spaces of these models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlNZAXzzWVCB"
   },
   "source": [
    "TODO : ???"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
