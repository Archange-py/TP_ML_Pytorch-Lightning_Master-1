{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXfaiY3oQxlA"
   },
   "source": [
    "# Unsupervised Learning and AutoEncoders.\n",
    "The goal of this notebook is to understand and handle AutoEncoders using Pytorch Lightning.\n",
    "\n",
    "It is recommended to run the notebook on Colab over a GPU device ([how to use a GPU on colab ](https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/colab.html)).\n",
    "\n",
    "Inspired by : https://deeplearning.neuromatch.io/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial1.html\n",
    "\n",
    "## Guidelines :\n",
    "Carefuly read the given code then\n",
    "*   complete cells marked with a TODO.\n",
    "*   answer questions marked with a QUESTION .\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "---\n",
    "### 1) Initialisation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3erI39SzQqDd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCJ_pBNsRDEH",
    "outputId": "02053064-a723-4d24-b3ea-125809de5fc4"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Set numpy seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set torch seeds\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set device:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRdN_G6bRGr3"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "### 1.1) The dataset\n",
    "---\n",
    "\n",
    "We are going to apply our AutoEncoder to the MNIST dataset, we can start by downloading it from Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzg1ih-kRFQw",
    "outputId": "44883b11-e61d-40b1-e708-e775772852f3"
   },
   "outputs": [],
   "source": [
    "#### Downloading dataset and creating Torch Dataloaders:\n",
    "\n",
    "# Download the MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root = \"./data\",\n",
    "                         train = True,\n",
    "                         download = True,\n",
    "                         transform = transforms.ToTensor())\n",
    "val_dataset = datasets.MNIST(root = \"./data\",\n",
    "                         train = False,\n",
    "                         download = True,\n",
    "                         transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tewfuAjLRUDC"
   },
   "source": [
    "The following list will be useful for plotting specific digits (see 3) Generative Models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "W2wArv2ZRXS4",
    "outputId": "4209d3d7-7c4d-4eef-9013-848dec0a7165"
   },
   "outputs": [],
   "source": [
    "all_digits_idx = []\n",
    "for digit in range(10):\n",
    "  idx_image = 0\n",
    "  while val_dataset[idx_image][1] != digit :\n",
    "    idx_image+=1\n",
    "  all_digits_idx.append(idx_image)\n",
    "\n",
    "fig,axs = plt.subplots(1,10, sharey=True, figsize=(16,8))\n",
    "for i,idx in enumerate(all_digits_idx):\n",
    "  axs[i].imshow(val_dataset[idx][0].view(28,28))\n",
    "  axs[i].set_title(f\"Label: {val_dataset[idx][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7Mlg__QRlnt"
   },
   "source": [
    "Note that for the rest of this notebook, we will be exploring the realm of Unsupervized Learning approaches. Althugh MNIST is a dataset comrpised of images as well as lassociated labels, we will intentionally disregard the labels, and focus solely on the pixel information. \n",
    "\n",
    "With this in mind, the goal of the notebook is to explore how we can work with only pixel values. Several possibilities arise, we focus on AutoEncoders and the concept of *Latent Spaces*. \n",
    "\n",
    "Without loss of generality, an AutoEncoder is comprised of **encoder**, a **bottleneck** and a **decoder** :\n",
    "*  The encoder maps data from a high-dimensional input to the bottleneck where the sub-space is the smallest.\n",
    "* The decoder converst the encoded input back to the original input space.\n",
    "* The space of the bottleneck is what we commonly refer to as the **latent space**. It is usually much smaller than the input space, but more informative. It can be seen as a compressed version of the image space in our case. Usually AutoEncoders look something like this:\n",
    "\n",
    "![AE.webp](data:image/webp;base64,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)\n",
    "\n",
    "\n",
    "(Image from [this article](https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df))\n",
    "\n",
    "Thoughouth the notebook, ty to have a look at this figure and identify the different components of our networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3Y7B0iuRtI7"
   },
   "source": [
    "---\n",
    "### 1.2) Helper Functions\n",
    "---\n",
    "\n",
    "Take time to understand and complete the following helper functions. What do they do?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEh-0zDhR1vc"
   },
   "outputs": [],
   "source": [
    "def plot_samples(autoencoder, dataset, num_to_plot=10):\n",
    "  fig, axs = plt.subplots(2,10, figsize=(80,10))\n",
    "  for i in range(num_to_plot):\n",
    "    sample = dataset[np.random.randint(len(dataset))][0].view(-1,1,28,28)\n",
    "    reconstruction = autoencoder(sample)\n",
    "\n",
    "    axs[0][i].imshow(sample.view(28, 28))\n",
    "    axs[1][i].imshow(reconstruction.detach().view(28, 28))\n",
    "  plt.subplots_adjust(wspace=0, hspace=0)\n",
    "  plt.show()\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce3KwOpyVGhX"
   },
   "source": [
    "---\n",
    "### 1.3 Hyperparameters tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHQJIoWpR71V"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters for all of our models. You may change this at will after running the notebook\n",
    "\n",
    "latent_dim = 20 # Size of our latent dimension\n",
    "batch_size = 64 \n",
    "num_epochs = 15\n",
    "learning_rate = 1e-3\n",
    "weight_decay_value = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQi7xqEUR9bv"
   },
   "source": [
    "---\n",
    "## 2) The Models\n",
    "---\n",
    "\n",
    "We can begin by simply implementing a Linear AutoEncoder. It is composed of two fully connected layers, one for encoding our input and one for decoding our latent space.\n",
    "\n",
    " In order to ensure strict linearity for now, no activation functions are used.\n",
    "\n",
    "---\n",
    " ### 2.1) Linear AutoEncoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqQOZv-ISKGK"
   },
   "outputs": [],
   "source": [
    "class LinearAutoEncoder(L.LightningModule):\n",
    "  def __init__(self, latent_dim):\n",
    "    super().__init__()\n",
    "    self.training_step_outputs = []\n",
    "\n",
    "    # Encoder layer (a linear mapping from input size to hidden_dimension)\n",
    "    self.linear_encoder = nn.Linear(28 * 28, latent_dim)\n",
    "\n",
    "    # Decoder layer (a linear mapping from hidden_dimension to input size)\n",
    "    self.linear_decoder = nn.Linear(latent_dim, 28 * 28)\n",
    "\n",
    "  def encode(self, x):\n",
    "    # Encode the input sample\n",
    "    latent = self.linear_encoder(x.view(x.size(0), -1))\n",
    "    return latent\n",
    "\n",
    "  def decode(self, latent):\n",
    "    # Decode the latent vector\n",
    "    x_prime = self.linear_decoder(latent).view(-1,1,28,28)\n",
    "    return x_prime\n",
    "\n",
    "  def forward(self, x):\n",
    "    latent = self.encode(x)\n",
    "    return self.decode(latent)\n",
    "\n",
    "  def backward(self, loss):\n",
    "    self.training_step_outputs.append(loss.detach().cpu().numpy())\n",
    "    loss.backward()\n",
    "\n",
    "  def training_step(self, batch):\n",
    "    x, y = batch\n",
    "    x = x.view(-1,1,28,28)\n",
    "    z = self.forward(x)\n",
    "    loss = F.mse_loss(z, y)\n",
    "    return loss\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0w7EgVxSOle",
    "outputId": "1dd5f62d-7482-4dc9-9a44-b058529ced68"
   },
   "outputs": [],
   "source": [
    "# Construct AE\n",
    "linear_ae = LinearAutoEncoder(latent_dim)\n",
    "\n",
    "print(linear_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "0kSRfoa0SjG2",
    "outputId": "5e8f47e1-1b51-459c-839c-46749de5440d"
   },
   "outputs": [],
   "source": [
    "##### Train model:\n",
    "\n",
    "### TODO:  Train the AE using the previously defined hyper-parameters ###\n",
    "\n",
    "#???\n",
    "\n",
    "### TODO: Plot the Reconstruction train and validation losses of the Linear AE ###\n",
    "\n",
    "#???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "Bhwqrh92SsMA",
    "outputId": "859fb3c5-0e4b-4bd9-91a0-c80074f6b439"
   },
   "outputs": [],
   "source": [
    "# Plot some reconstructions:\n",
    "plot_samples(linear_ae, val_dataset, num_to_plot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A9KG_5YSvSv"
   },
   "source": [
    "#### QUESTION\n",
    " What do you think of these sample reconstructions? What easy steps could you implement in order to obtain more accurate reconstructions?\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjfV6kj4S2Jw"
   },
   "source": [
    "TODO: They are not visually pleasing or accurate, but are still impressive for such a basic model. The model can clearly differentiate between digits, but inter-digit variation seems to be blurry. We could add more layers to the encoder and decoder , but activation functions would then be necessary and thus the model would not be linear strictly speaking. Furthermore, an output activation could help force the network to at least get the correct intensity range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5-fSQnMS5Y9"
   },
   "source": [
    "---\n",
    "### 2.2) Convolutional AutoEncoder (Non Linear)\n",
    "---\n",
    "In order to implement a Convolutional AutoEncoder, we will be using Transpose Convolutions in order to upsample our latent space.\n",
    "\n",
    "#### QUESTION\n",
    "\n",
    " Using [Pytorch's documentation](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) and [this explanation](https://d2l.ai/chapter_computer-vision/transposed-conv.html), explain in your own words how a Transpose convolution works. Does it correspond to a \"deconvolution\" in the sens of the inverse of the convolution operator?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3o2BN9JTHeZ"
   },
   "source": [
    "TODO: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRXjCy3ZTL_i"
   },
   "source": [
    "The following is an example implementation of a Convolutional AE using pytorch. It takes an MNIST image (1,28,28) image, convolves it to a (64,1,1) feature map\n",
    "\n",
    "*   It takes an MNIST image (1,28,28) image,\n",
    "*   Convolves it to a (64,1,1) feature map,\n",
    "*   Flattens the feature map, into a vector of shape 64,\n",
    "*   Reduces this to the given latent_dimension size,\n",
    "*   Then decodes the latent vector back to an image.\n",
    "\n",
    "Complete the class implementation in order to have an AE which functions as aforementioned. (You may want to use [Pytroch's documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) for a refresher on convolutional arithmetic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9Il7mRJSuDR"
   },
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.training_step_outputs = []\n",
    "        # Input (batch_size, 1, 28, 28)\n",
    "        self.encoder = nn.Sequential(\n",
    "            ### TODO: Complete the missing values ###\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), # (batch_size, 16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # (batch_size, 32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7), # (batch_size, 64, 1, 1)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "            )\n",
    "\n",
    "        ### TODO: Complete the missing values ###\n",
    "        self.fc_encoder = nn.Linear(64, latent_dim)\n",
    "        self.fc_decoder = nn.Linear(latent_dim, 64)\n",
    "\n",
    "\n",
    "        #64, 1, 1\n",
    "        self.decoder = nn.Sequential(\n",
    "            ### TODO: Complete the missing values ###\n",
    "            nn.Unflatten(dim=-1, unflattened_size=(64,1,1)),\n",
    "            nn.ConvTranspose2d(64, 32, 7), # (batch_size, 32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # (batch_size, 16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), # (batch_size,1, 28, 28)\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def encode(self, x):\n",
    "        ### TODO: Complete the encode method ###\n",
    "        x = self.encoder(x)\n",
    "        latent = self.fc_encoder(x)\n",
    "        return latent\n",
    "\n",
    "    def decode(self, latent):\n",
    "      ### TODO: Complete the decode method ###\n",
    "        s = F.relu(self.fc_decoder(latent))\n",
    "        x_prime = self.decoder(s)\n",
    "        return x_prime\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encode(x)\n",
    "        x_prime = self.decode(latent)\n",
    "        return x_prime\n",
    "\n",
    "    def backward(self, loss):\n",
    "      self.training_step_outputs.append(loss.detach().cpu().numpy())\n",
    "      loss.backward()\n",
    "\n",
    "    def training_step(self, batch):\n",
    "      x, y = batch\n",
    "      x = x.view(-1,1,28,28)\n",
    "      z = self.forward(x)\n",
    "      loss = F.mse_loss(z, y)\n",
    "      return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "      optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "      return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtrg6zCiTdF9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M40r63-fTdTD"
   },
   "source": [
    "#### QUESTION\n",
    " Why is the final activation of our network a Sigmoid? What other activation functions could we have used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXg8v3NaTkNs"
   },
   "source": [
    "TODO : ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zNE-72HdSyj5",
    "outputId": "e8373257-737f-4f11-c722-26a4c471e313"
   },
   "outputs": [],
   "source": [
    "# Construct AE\n",
    "conv_ae = ConvAutoEncoder(latent_dim)\n",
    "\n",
    "print(linear_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "ieBuwvTJTthf",
    "outputId": "56ca8b41-1c4c-485e-a500-ce98f96d1202"
   },
   "outputs": [],
   "source": [
    "##### Train model:\n",
    "\n",
    "### TODO:  Train the AE using the previously defined hyper-parameters ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hd5bNMFTzag"
   },
   "source": [
    "#### QUESTION\n",
    " Which of both models performs better? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3awnAgLMT2xN"
   },
   "source": [
    "TODO : ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "ikpBQx-4T6in",
    "outputId": "8049c07f-13d8-49b5-feaf-07f9c81ef107"
   },
   "outputs": [],
   "source": [
    "# Plot some reconstructions:\n",
    "plot_samples(conv_ae, val_dataset, num_to_plot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-k6zoo3T92k"
   },
   "source": [
    "\n",
    "---\n",
    "### 2.3) Variational AutoEncoder (VAEs)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUqwtRnAUAhp"
   },
   "outputs": [],
   "source": [
    "class VAE(L.LightningModule):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.training_step_outputs = []\n",
    "        # Input: (batch_size, 1, 28, 28)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), #(batch_size, 16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), #(batch_size, 32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7), #(batch_size, 64, 1, 1)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "            )\n",
    "\n",
    "\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.fc_decoder = nn.Linear(latent_dim, 64)\n",
    "\n",
    "\n",
    "        # Input: (batch_size, 64, 1, 1)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(dim=-1, unflattened_size=(64,1,1)),\n",
    "            nn.ConvTranspose2d(64, 32, 7), #(batch_size, 32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), #(batch_size, 16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), #(batch_size, 1, 28, 28)\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + (std * epsilon)\n",
    "        return z\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Get mu and logvar or reparam trick\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "\n",
    "        latent = self.reparameterize(mu, logvar)\n",
    "        return latent, mu, logvar\n",
    "\n",
    "    def decode(self, latent):\n",
    "        s = F.relu(self.fc_decoder(latent))\n",
    "        x_prime = self.decoder(s)\n",
    "        return x_prime\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent, mu, logvar = self.encode(x)\n",
    "\n",
    "        x_prime = self.decode(latent)\n",
    "        return x_prime, mu, logvar\n",
    "\n",
    "    def backward(self, loss):\n",
    "      self.training_step_outputs.append(loss.detach().cpu().numpy())\n",
    "      loss.backward()\n",
    "\n",
    "    @staticmethod\n",
    "    def VAE_loss_fn(mse_loss, mu, logvar, beta=1):\n",
    "        KL_Div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return mse_loss + beta*KL_Div\n",
    "\n",
    "    def training_step(self, batch):\n",
    "      x, y = batch\n",
    "      x = x.view(-1,1,28,28)\n",
    "      z = self.forward(x)\n",
    "      loss = VAE.VAE_loss_fn(F.mse_loss(z[0], y), z[1], z[2])\n",
    "      return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "      optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "      return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmnsNUKMUFaf"
   },
   "source": [
    "The following is a re-implementation of the train function for the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgTTlQBHUO_g",
    "outputId": "f0cd2a48-cafe-416d-8d50-f2fca3a08273"
   },
   "outputs": [],
   "source": [
    "# Construct AE\n",
    "vae_model = VAE(latent_dim)\n",
    "\n",
    "print(vae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "u91Z4l5vUcM6",
    "outputId": "76c5161a-489c-4ec6-a955-f120fc7de62f"
   },
   "outputs": [],
   "source": [
    "#### Train VAE\n",
    "\n",
    "### TODO:  Train the VAE using the previously defined hyper-parameters ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R831DUjOUf4b"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### QUESTION\n",
    " Why does the plotted Loss seem to indicate that the VAE has a terrible reconstruction compared to the previous models? Is it the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpjRdJUwUjGx"
   },
   "source": [
    "TODO: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "EmraEMejUq0a",
    "outputId": "ddcd83f2-50ed-4bed-b622-51ba15a42a2a"
   },
   "outputs": [],
   "source": [
    "#Read the plot_samples helper function again, and complete the script in order to compare sample reconstructions from all three trained models  ###\n",
    "\n",
    "fig, axs = plt.subplots(4,10, figsize=(60,10))\n",
    "criterion = nn.MSELoss()\n",
    "for i in range(10):\n",
    "  # Get a random sample from the MNIST validation set.\n",
    "  sample = val_dataset[np.random.randint(len(val_dataset))][0].view(-1,1,28,28)\n",
    "\n",
    "  # Forward through trained models to get reconstructions.\n",
    "  reconstruction_linear_ae = linear_ae(sample) # TODO\n",
    "  reconstruction_conv_ae = conv_ae(sample) # TODO\n",
    "  reconstruction_vae, _, _ = vae_model(sample) # TODO\n",
    "\n",
    "  # Evaluate the reconstruction error.\n",
    "  lin_ae_mse = criterion(reconstruction_linear_ae,sample).item() # TODO\n",
    "  conv_ae_mse = criterion(reconstruction_conv_ae,sample).item() # TODO\n",
    "  vae_mse = criterion(reconstruction_vae,sample).item() # TODO\n",
    "\n",
    "  # Display everything\n",
    "  axs[0][i].imshow(sample.view(28, 28))\n",
    "  axs[1][i].imshow(reconstruction_linear_ae.detach().view(28, 28))\n",
    "  axs[1][i].set_title(str(lin_ae_mse))\n",
    "  axs[2][i].imshow(reconstruction_conv_ae.detach().view(28, 28))\n",
    "  axs[2][i].set_title(str(conv_ae_mse))\n",
    "  axs[3][i].imshow(reconstruction_vae.detach().view(28, 28))\n",
    "  axs[3][i].set_title(str(vae_mse))\n",
    "#plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVJ6B9lyWBvR"
   },
   "source": [
    "---\n",
    "## 3) Autoencoders as generative models\n",
    "---\n",
    "\n",
    "AutoEncoders are not only useful for dimensionality-reduction. They also encapsulate generating capabilities, and sometimes display interesting properties in terms of their Latent Space.\n",
    "\n",
    "#### QUESTION\n",
    " What does the following function do?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvvhQLlFWOml"
   },
   "source": [
    "TODO : ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "5cinKrBUWIk7",
    "outputId": "c57cf4e2-b0d0-499c-f7d6-3b2d205b8940"
   },
   "outputs": [],
   "source": [
    "def interpolate(ae, x_1, x_2, n=12):\n",
    "    if not isinstance(ae,VAE):\n",
    "      z_1 = ae.encode(x_1.view(-1,1,28,28))\n",
    "      z_2 = ae.encode(x_2.view(-1,1,28,28))\n",
    "    else:\n",
    "      z_1, mu_1, logvar_1 = ae.encode(x_1.view(-1,1,28,28))\n",
    "      z_2, mu_2, logvar_2 = ae.encode(x_2.view(-1,1,28,28))\n",
    "    image = np.zeros((28,28*n))\n",
    "    for i, t in enumerate(np.linspace(0, 1, n)):\n",
    "      if not isinstance(ae,VAE):\n",
    "        recons = ae.decode(z_1 + (z_2 - z_1)*t).view(28,28).detach().numpy()\n",
    "      else:\n",
    "        mu = mu_1 + (mu_2 - mu_1)*t\n",
    "        logvar = logvar_1 + (logvar_2 - logvar_1)*t\n",
    "        latent = ae.reparameterize(mu, logvar)\n",
    "        recons = ae.decode(latent).detach().numpy()\n",
    "      image[:, 28*i:28*(i+1)] = recons\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "idx_1, idx_2 = all_digits_idx[4], all_digits_idx[6]\n",
    "img_1, img_2 = val_dataset[idx_1][0], val_dataset[idx_2][0]\n",
    "\n",
    "interpolate(linear_ae, img_1, img_2)\n",
    "interpolate(conv_ae, img_1, img_2)\n",
    "interpolate(vae_model, img_1, img_2)\n",
    "\n",
    "# interpolate(model,image[0,:,:,:],image[3,:,:,:])\n",
    "# interpolate(model,image[3,:,:,:],image[2,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PXP3lF4WU8l"
   },
   "source": [
    "#### QUESTION\n",
    " What can you say abouth the latent spaces of these models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlNZAXzzWVCB"
   },
   "source": [
    "TODO : ???"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
